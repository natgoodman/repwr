---
title: "Noncentral d2t-Distribution Applied to Measures of Replication Success"
author: "Nathan (Nat) Goodman"
date: "December 02, 2018"
output:
  pdf_document: default
  html_document:
    css: css/html_document.css
    highlight: kate
linkcolor: cyan
citecolor: green
urlcolor: blue
---

*Statisticians have devised numerous statistical tests for deciding whether a replication passes or fails, thus validating of refuting the original result. I call these tests "measures". Elsewhere I report results from simulating these tests and describe the measures in detail. Here I discuss nuances of the noncentral t-distribution that underlie many replication measures.*

## Introduction

Various authors have proposed tests for deciding whether a replication succeeds or fails. I simulated many of rules across a range of replication conditions and reported results in a working paper [Systematic Replication Has Limited Power to Detect Bad Science](https://natgoodman.github.io/repwr/repwr.stable.html), a shorter blog post [Systematic Replication May Make Many Mistakes](https://replicationnetwork.com/2018/09/28/goodman-systematic-replication-may-make-many-mistakes/), and a [supplement to the blog post](https://natgoodman.github.io/repwr/resigsupp.stable.html). I described the measures themselves in a working paper [Measures of Replication Success](https://natgoodman.github.io/repwr/mesr.stable.html).

Several measures depends on nuances of the *noncentral t-distribution*. Introductory statistics material usually parameterize the central t-distribution by *degrees of freedom*; the noncentral t-distribution also has a *noncentrality parameter*; and both operate on the *t-statistic*. I prefer to work with more concrete parameters: *sample size* and *standardized observed effect size* (aka *Cohen’s d*). For the limited context of my simulation, it's easy to convert between the concepts.

Here I represent my software for the noncentral t-distribution parameterized by sample size and effect size. I call this the *d2t-distribution*. I must emphasize that the d2t-distribution is not a new distribution in any sense; it's merely a repackaging of the t-distribution.

## Simulation 

I consider a basic replication scheme in which each original study is repeated once. 

The software first simulates *studys* across a range of conditions, then combines pairs of studies into *pairwise replications*, applies rules (the *measures*) for deciding which pairwise replications pass, and finally computes true and false positive and negative rates for measures and conditions of interest.

The studies are simple two group comparisons parameterized by sample size $n$ and population effect size $d_{pop}$ ($d_{pop}\ge0$). For each study, I generate two groups of random numbers, each of size $n$.  One group, *group0*, comes from a standard normal distribution with $mean=0$; the other, *group1*, is from a standard normal distribution with mean $d_{pop}$. When I need to be pedantic, I use the term *study set* for the ensemble of studies for a given combination of $n$ and $d_{pop}$. 

To generate pairwise replications, I consider all (ordered) pairs of study sets. For each pair, the software permutes the instances of each study, then combines the instances row-by-row. A *pairwise replication set* is the ensemble of pairwise replication instances for a given pair of study sets. Four variables parameterize each pairwise replication set: $n1$, $n2$, $d1_{pop}$, $d2_{pop}$. These are, naturally enough, the sample and population effect sizes for the two study sets.

After forming the pairwise replications, I apply the measures. The result is a boolean matrix whose rows represent replication instances and columns represent the measures' results.

## Noncentral d2t-Distribution

The definition of several measures depends on nuances of the t-distribution. I learned the hard way that the classic (*central*) t-distribution only applies when the NULL is true. When the NULL isn't true, the correct sampling distribution is the *noncentral t-distribution*. 

The usual treatment parameterizes the central t-distribution by *degrees of freedom*; the noncentral t-distribution also has a *noncentrality parameter*; and both operate on the *t-statistic*. I'm sure there are good reasons for these mysterious concepts, but I prefer the more concrete parameters *sample size* and *standardized observed effect size* (aka *Cohen’s d*). For the limited context of this simulation, it's easy to convert between the concepts. While I'm at it, I define conversions between p-values and both my prefered units and t-statistics.

In the following math, the notation is

  - degrees of freedom: $df$ (`df`)
  - sample size: $n$ (`n`)
  - standardized observed effect size: $d_{sdz}$
  - t-statistic: $t$
  - p-value: $pval$
  - noncentrality parameter: $ncp$

**Notation when working with individual studies**

| math | code | meaning |
| ------- | ------- | -------------------------------------------------------- |
| $df$ | `df` | degrees of freedom |
| $n$ | `n` | sample size |
| $d_{sdz}$ | `d` | standardized observed effect size, aka *Cohen’s d*; I use `d` in this code, instead of `d.sdz`, to save space since there's no possible ambiguity |
| $t$ | `t` | t-statistic |
| $pval$ | `pval` | p-value |
| $ncp$ | `ncp` | noncentrality parameter expressed in $t$ units |
| $d0$ | `d0` | noncentrality parameter expressed in $d_{sdz}$ units |
| $\alpha$ | `sig.level` | significance level | 
| $c$ | `conf.level` | confidence level for confidence intervals | 
| $p$ | `pred.level` | confidence level for prediction intervals | 
| $d_{sig}$ | `d.sig` | minimum significant effect size for a sample size of $n$ |



**Additional notation when working with pairs of studies, i.e., replications**

| $c$ | `conf.level` | confidence level for confidence intervals | 


**Specialized notation for Uri Simonsohn's small telescope method**

  - sample sizes of the two studies: $n1$ (`n1`), $n2$ (`n2`)
  - required power of original study: $pwr$ (`scope.power`); Uri fixes this at 33%
  - minimum significant effect size for a sample size of $n1$: $d_{sig}$ (`d.sig`)
  - significance level: $\alpha$ (`scope.alpha`)
  - minimum population effect size that would confer $pwr$ power to the original study: $d_{pwr}$ (`d.pwr`); Uri calls this $d_{33\%}$
  - minimum acceptable observed effect size in the replica: $d_{scope}$ (`d.scope`)
  - noncentrality parameter in $d$ units: $d0$ (`d0`)




\begin{aligned}
&\bf{\text{sample size to degrees of freedom}} \\
df&=2(n-1) \\

&\bf{\text{t-statistic to Cohen's d and p-value. Note: pt() is from base R.}} \\
d_{sdz}&=t \sqrt{(2n)/n^2} \\ 
pval&=2 \times pt(-\lvert t\rvert,df=2*(n-1)) \\

&\bf{\text{Cohen's d to t-statistic, p-value, and noncentrality parameter. Note: pt() is from base R.}} \\
t&=d_{sdz} \sqrt{n^2/2n} \\
pval&=2 \times pt(-\lvert d_{sdz}\rvert \sqrt{n^2/2n}),df=2*(n-1)) \\
ncp&=d_{sdz} \sqrt{n/2} \\

&\bf{\text{p-value to t-statistic and Cohen's d. Note: qt() is from base R, q_d2t() is wrapper for qt().}} \\
t&=qt(pval/2,df=2*(n-1),lower.tail=F) \\
d_{sdz}&=q\_d2t(n,q=pval/2,lower.tail=F) \\
\end{aligned}

I use these transformations to define a family of `d2t` functions analogous to R's `t` functions, plus a few more. These function are in the file `R/stats.R`. In the code,

  - degrees of freedom: `df`
  - sample size (of each group): `n`
  - standardized observed effect size: `d`; I use `d` in this code, instead of `d.sdz`, to save space since there's no possible ambiguity
  - noncentrality parameter in `d` units: `d0`
  - t-statistic: `t`
  - p-value: `pval`
  - noncentrality parameter in `t` units: $ncp$

The first group are basic transformations.

``` {r eval=FALSE}
## t-statistic to Cohen's d & p-value
t2d=function(n,t) t*sqrt((2*n)/n^2)
t2pval=function(n,t) 2*pt(-abs(t),df=2*(n-1))

## Cohen's d to t-statistic, p-value, and noncentrality parameter
d2t=function(n,d) d*sqrt(n^2/(2*n))
d2pval=function(n,d) t2pval(n,d2t(n,d))
ncp=function(n,d) sqrt(n/2)*d

## p-value to t-statistic & Cohen's d
pval2t=function(n,pval) qt(pval/2,df=2*(n-1),lower.tail=F)
pval2d=function(n,pval) q_d2t(n,q=pval/2,lower.tail=F)        # see below for q_d2t
```

The next group are *d2t* wrappers around R's t-distribution functions. They make liberal use of the transformations above.

``` {r eval=FALSE}
## density function of t-distribution in concrete units (n, d)
d_d2t=function(n,d,d0=NULL) {
  df=2*(n-1);
  t=d2t(n,d);
  if (!is.null(d0)) suppressWarnings(dt(t,df=df,ncp=ncp(n,d0)))
  else dt(t,df=df)
}
## distribution function of t-distribution in concrete units (n, d)
p_d2t=function(n,d,d0=NULL,lower.tail=TRUE) {
  df=2*(n-1);
  t=d2t(n,d);
  if (!is.null(d0)) suppressWarnings(pt(t,df=df,ncp=ncp(n,d0),lower.tail=lower.tail))
    else pt(t,df=df,lower.tail=lower.tail)
}
## quantile function of t-distribution in concrete units (n, d)
q_d2t=function(n,q,d0=NULL,lower.tail=TRUE) {
  df=2*(n-1);
  if (!is.null(d0)) t=suppressWarnings(qt(q,df=df,ncp=ncp(n,d0),lower.tail=lower.tail))
    else t=qt(q,df=df,lower.tail=lower.tail)
  t2d(n,t);
}
## random generation function of t-distribution in concrete units (n, d)
r_d2t=function(m,n,d0=NULL) {
  df=2*(n-1);
  if (!is.null(d0)) t=suppressWarnings(rt(m,df=df,ncp=ncp(n,d0))) else t=rt(m,df=df);
  t2d(n,t)
}
```

The third group compute the mean and standard deviation of the d2t-distribution. I was surprised to learn that base R doesn't provide these functions for the standard t-distribution. Note the comments about R's `gamma` function and the workaround using `lgamma`. I'm sure I didn't devise this workaround on my own but my notes don't say where I found it.  Sorry!

``` {r eval=FALSE}
## mean of t-distribution in concrete units (n, d) 
mean_d2t=function(n,d0=NULL) {
  df=2*(n-1);
  if (!is.null(d0)) {
    ncp=ncp(n,d0);
    ## Note: gamma blows up when n>100 or so. use lgamma instead
    ##       theo.mean=sqrt(df/2)*ncp*gamma((df-1)/2)/gamma(df/2)
    theo.mean=sqrt(df/2)*ncp*exp(lgamma((df-1)/2)-lgamma(df/2))
    t2d(n,theo.mean)
  } else 0;
}
## standard deviation of t-distribution in concrete units (n, d) 
sd_d2t=function(n,d0=NULL) {
  df=2*(n-1);
  if (!is.null(d0)) {
    ncp=ncp(n,d0);
    ## Note: gamma blows up when n>100 or so. use lgamma instead
    ##       theo.mean=sqrt(df/2)*ncp*gamma((df-1)/2)/gamma(df/2)
    theo.mean=sqrt(df/2)*ncp*exp(lgamma((df-1)/2)-lgamma(df/2))
    theo.var=(1+ncp^2)*df/(df-2)-(theo.mean^2)
    theo.sd=sqrt(theo.var)
    t2d(n,theo.sd)
  } else
    (sqrt(2*n)/n)*sdt(2*(n-1));
}
## standard deviation of central t-distribution in concrete units (n, d)
sdt=function(df) sqrt(df/(df-2))
```

The final group calculate confidence and prediction intervals for the d2t-distribution.

**Confidence intervals**

The bounds of the confidence interval are the smallest and largest *population* effect sizes for which the observed effect size $d_{sdz}$ would be significant. I learned about confidence intervals for Cohen's d from Uri Simonsohn's blogpost [We cannot afford to study effect size in the lab](http://datacolada.org/20) and adapted the confidence interval function from [Uri's code](http://urisohn.com/sohn_files/BlogAppendix/Colada20.ConfidenceIntervalsForD.R) accompanying that post.

In the following math, the notation is

  - sample size (of each group): $n$
  - standardized observed effect size: $d_{sdz}$
  - lower and upper bounds of confidence interval: $d_{lo}$, $d_{hi}$; these are the smallest and largest *population* effect sizes for which $d_{sdz}$ would be significant
  - lower and upper probability cutoffs for confidence interval: $p_{lo}$, $p_{hi}$
  - noncentrality parameter in $d$ units: $d0$
  - confidence level: $c$
  - $p_d2t()$ is a wrapper for R's `pt()` that expresses the t-distribution cumulative probability in $d$ units.

\begin{aligned}
p0&=c/2 \\
p1&=1-p0 \\
d_{lo}&=d0 \mid p_d2t(n,d_{sdz},d0)=p_{lo} \\
d_{hi}&=d0 \mid p_d2t(n,d_{sdz},d0)=p_{hi} \\
\end{aligned}

In the code,

  - sample size (of each group): `n`
  - standardized observed effect size: `d`; I use `d` in this code, instead of `d.sdz`, to save space since there's no possible ambiguity
  - lower and upper bounds of confidence interval: `d.lo`, `d.hi`
  - lower and upper probability cutoffs for confidence interval: `p.lo`, `p.hi`
  - noncentrality parameter in `d` units: `d0`
  - confidence level: `conf.level`

The code uses R's `uniroot` function to solve the equations for $d_{lo}$ and $d_{hi}$.
  
``` {r eval=FALSE}
## adapted from http://urisohn.com/sohn_files/BlogAppendix/Colada20.ConfidenceIntervalsForD.R
ci_d2t=function(n,d,conf.level=0.95) {
  p.lo=(1-conf.level)/2; p.hi=1-p.lo;
  d.lo=suppressWarnings(
    uniroot(function(d0) p_d2t(n,d,d0,lower.tail=F)-p.lo,interval=c(-10,10))$root);
  d.hi=suppressWarnings(
    uniroot(function(d0) p_d2t(n,d,d0,lower.tail=F)-p.hi,interval=c(-10,10))$root);
  c(d.lo,d.hi);
}
```

**Prediction intervals**

A prediction interval indicates the range of likely effect sizes in a replication, given the sample sizes of the original and replica studies and the observed effect size in the original study.

I adapted the prediction interval function from [Spence and Stanley's paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5028066/) and [Stanleys predictionInterval package](https://cran.r-project.org/web/packages/predictionInterval/). The code seems to work, but I have no intuition why.

In the code, 

  - suffixes `1` and `2` denote the original and replica studies, respectively
  - sample sizes of the two studies: `n1`, `n2`
  - standardized observed effect size in the original study: `d`; I use `d` in this code, instead of `d.sdz`, to save space since there's no possible ambiguity
  - confidence intervals for the two studies, assuming the replica observes the same effect size as the original: `ci1`, `ci2`

``` {r eval=FALSE}
## adapted from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5028066/ and predictionInterval package
pi_d2t=function(n1,n2,d,ci1=NULL,ci2=NULL,pred.level=0.95) {
  if (is.null(ci1)) ci1=ci_d2t(n1,d);
  if (is.null(ci2)) ci2=ci_d2t(n2,d);
  l1=ci1[1]; u1=ci1[2];
  l2=ci2[1]; u2=ci2[2];
  c(d-sqrt((d-l1)^2+(u2-d)^2),d+sqrt((d-l2)^2+(u1-d)^2));
}
```

## Implementation

Performance is critical, because I apply the measures to many, many replications (more than 100 million in default full runs). The code computes what it can before doing any simulations, then what it can on each simulated study individually, then values that depend only on the replication parameters ($n1$, $n2$, $d1_{pop}$, $d2_{pop}$), and finally statistics that depend on pairs of studies.  Once each replication set is formed, the software implements the measures as vectorized in-line code (i.e., not as functions) that operates on the entire set in a single gulp. 

I now turn to specific measures and statistics used by measures.

### Values computed before simulation

Values computed before the simulation cannot depend on the data, of course. In addition to values that truly do not depend on the data, I pre-compute expensive statistics on a parameter grid, then later interpolate to get the values for a specific study or replication. 

#### Small telescope thresholds

Uri Simonsohn's small telescope thresholds depend only on the sameple sizes of the original and replica studies. In Uri's words, the method "evaluates the original study’s *design* rather than its result — the telescope rather than the planet".

**Definition**

The method first computes the smallest population effect size that would confer 33% power to the original study; Uri calls this $d_{33\%}$. Then it computes the smallest observed effect size that the replica study could see consistent with the hypothesis that the true effect size is $\ge d_{33\%}$. I call this $d_{scope}$. If the replica observes an effect size $\lt d_{scope}$, the method concludes that the population effect size is too small to have been really detected by the original study.

**Notation:** $math$ (`code`)

  - sample sizes of the two studies: $n1$ (`n1`), $n2$ (`n2`)
  - required power of original study: $pwr$ (`scope.power`); Uri fixes this at 33%
  - minimum significant effect size for a sample size of $n1$: $d_{sig}$ (`d.sig`)
  - significance level: $\alpha$ (`scope.alpha`)
  - minimum population effect size that would confer $pwr$ power to the original study: $d_{pwr}$ (`d.pwr`); Uri calls this $d_{33\%}$
  - minimum acceptable observed effect size in the replica: $d_{scope}$ (`d.scope`)
  - noncentrality parameter in $d$ units: $d0$ (`d0`)

**Functions from [Statistical Preliminaries](#statistical-preliminaries) section**

  - $pval2d$ converts p-value to a standardized effect size
  - $p_d2t()$ is a wrapper for R's `pt()` that expresses the t-distribution cumulative probability in $d$ units.

**Math**

\begin{aligned}
d_{sig}&=pval2d(n1,\alpha) \\
d_{pwr}&=d0 \mid p_d2t(n1,d_{sig},d0)=pwr \\
d_{scope}&=d \mid p_d2t(n2,d,d_{sig})=\alpha \\
\end{aligned}

**Code**

The code uses R's `uniroot` function to solve the equations for $d_{pwr}$ and $d_{scope}$.

``` {r eval=FALSE}
d.sig=pval2d(n1,scope.alpha);
d.pwr=uniroot(function(d0) p_d2t(n1,d=d.sig,d0,lower.tail=F)-scope.power,interval=c(0,10))$root;
## suppress annoying warning from pt: full precision may not have been achieved in 'pnt{final}'
d.scope=suppressWarnings(uniroot(function(d) p_d2t(n2,d,d0=d.pwr)-scope.alpha,c(-10,10))$root);
```

#### Effect size dependent small telescope thresholds

#### Confidence intervals

#### Prediction intervals



### Values computed on individual studies

In the code,

  - sample size (of each group): `n`
  - standardized observed effect size: `d.sdz`; I use `d.sdz` in this code to avoid confusion with the population effect size `d.pop`
  - p-value: `pval`

#### Standardized effect size, aka *Cohen’s d*

Notation: $d_{sdz}$ (`d.sdz`)

Definition: Mean of *group1* minus the mean of *group0* divided by the pooled standard deviation of the two groups.

Math: The standard formula for pooled standard deviation when the two groups have the same sample size $n$ and standard deviations $sd0$ and $sd1$ is 
\begin{aligned}
pooled.sd&=\sqrt{((n-1)sd0^2+(n-1)sd1^2)/(2n -2))} \\
&=\sqrt{(sd0^2+sd1^2)/2} \\
\end{aligned}
The overall formula for standardized effect size is
\begin{aligned}
d_{raw}&=mean(group0)-mean(group1) \\
d_{sdz}&=d.raw/pooled.sd \\
\end{aligned}

R:
``` {r eval=FALSE}
pooled_sd=function(sd0,sd1) sqrt((sd0^2+sd1^2)/2);
d.raw=mean(group0)-mean(group1);
d.sdz=d.raw/pooled_sd(sd(group0),sd(group1));
```

####  P-value computed from t-distribution 

Notation: $pval$ (`pval`)

Math: Computed from *n*, *d.sdz* by the transformations in [Statistical Preliminaries](#statistical-preliminaries)

R:
``` {r eval=FALSE}
## d2pval function in Statistical Notes section
pval=d2pval(n,d.sdz); 
```

#### Confidence interval

"ci.lo"  "ci.hi"


#### Absolute value and sign

Notation: $d_{abs}$ (`d.abs`), $d_{sign}$ (`d.sign`)

Definition: $d_{abs}$ is the absolute value of $d_{sdz}$. $d_{sign}$ is the sign of $d_{sdz}$: 1 for positive, -1 for negative, 0 for the very unlikely case that $d_{sdz}$ is exactly 0. The code pre-computes these for a very small performance gain.

Math: 
\begin{aligned}
d_{abs}&=\lvert d_{sdz}\rvert \\
d_{sign}&=\begin{cases}
1,  & \text{if $d_{sdz}$ > 0} \\
-1,  & \text{if $d_{sdz}$ < 0} \\
0, & \text{if $d_{sdz}$ = 0}
\end{cases}
\end{aligned}

R:
``` {r eval=FALSE}
d.abs=abs(d.sdz);
d.sign=sign(d.sdz);
```

####  Pre-computed values for fixed-effect meta-analysis

Notation: $w_{meta}$ (`w.meta`), $d_{meta}$ (`d.meta`).  

Definition: Inverse variance weight and weighted effect size of an individual study. The variance in question is the sampling variance of the estimated effect size ($d_{sdz}$), not the variance of the data! The sampling distribution of $d_{sdz}$ is a noncentral t-distribution centered on $d_{sdz}$ with $2(n-1)$ degrees of freedom. The weighted effect size is simply $d_{sdz}$ divided by the weight. The code pre-computes these values for each study, then combines them to yield the final meta-analytic statistics for pairs of studies.

Math: The standard deviation is computed from $n$ and $d_{sdz}$ by the $sd\_d2t$ function in [Statistical Notes](#statistical-notes). The variance is the square of this.

R: 
``` {r eval=FALSE}
## sd_d2t function in Statistical Notes section
w.meta=1/(sd_d2t(n,d.sdz)^2);
d.meta=d.sdz*w.meta;
```

`r knitr::knit_exit()`




**Computed on individual studies**
  - `d.sdz` - the *standardized observed effect size* $d_{sdz}$, aka *Cohen’s d*, as the mean of *group1* minus the mean of *group0* divided by the pooled standard deviation of the two groups
  - `sd` - pooled standard deviation
  - `pval` - p-value
  - inverse variance weight for fixed-effect meta-analysis
  - more TBD
  - "d.sdz"  "sd"     "pval"   "ci.lo"  "ci.hi"  "d.sign"
 "d.abs"  "w.meta" "d.meta"
  - `d.sdz` - the *standardized observed effect size* $d_{sdz}$, aka *Cohen’s d*, as the mean of *group1* minus the mean of *group0* divided by the pooled standard deviation of the two groups- the *standardized observed effect size* $d_{sdz}$, aka *Cohen’s d*, as the mean of *group1* minus the mean of *group0* divided by the pooled standard deviation of the two groups
  - `sd` - pooled standard deviation
  - `pval` - p-value from t-test
  - `ci.lo`, `ci.hi` - lower and upper bounds of 95% confidence interval. Implementation adapted from the [code](http://urisohn.com/sohn_files/BlogAppendix/Colada20.ConfidenceIntervalsForD.R) for Uri Simonsohn's blog post [We cannot afford to study effect size in the lab](http://datacolada.org/20). See code below.
  - `d.sign` - sign of `d.sdz`
  - `d.abs` - absolute value of `d.sdz`
  - `w.meta` - inverse variance weight for fixed-effect meta-analysis. see code below
  - `d.meta` - weighted effect size, `d.sdz * w.meta`
  
**Computed from replication parameters**
  - more TBD
  

; my implementation of confidence intervals is based on [Uri's Simonsohn's code](http://urisohn.com/sohn_files/BlogAppendix/Colada20.ConfidenceIntervalsForD.R) supporting his post [We cannot afford to study effect size in the lab](http://datacolada.org/20)

; the method is from a [paper by Jeffrey Spence and David Stanley](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5028066/) with code adapted from the [Stanley's predictionInterval package](https://cran.r-project.org/web/packages/predictionInterval)

; the method is from Uri's [Small Telescopes paper](http://datacolada.org/wp-content/uploads/2015/05/Small-Telescopes-Published.pdf) and post [Accepting the Null: Where to Draw the Line?](http://datacolada.org/42) with code adapted from [Uri's code](http://urisohn.com/sohn_files/BlogAppendix/Colada42/%5b42%5d%20-%20Accepting%20the%20Null%20-%20R%20-%20Reproducibility%20project%20small%20telescope%20vs%20bayesian%202015%2010%2026.R) supporting that post

- *scpd1*, *scpd2* are my extension to Uri's small telescope method that takes into account the observed effect size; the method arose from my misunderstanding of Uri's method; since I had the code, I decided to include it here

>>> perhaps include some code here <<<


## Nomenclature

I've already introduced the nomenclature in the XXX section.  Here is a concise list.

For studies
- *study instance* is a single simulated study
- $n$ is the sample size
- $d_{pop}$ is the population effect size
- $d_{sdz}$ is the standardized observed effect size (aka Cohen’s $d$)
- *study set* is the set of study instances for a given $n$ and $d_{pop}$

For pairwise replications, I add the letters *1*, *2* to mean parameters or statistics from the first or second study of the pair respectively.
- *pairwise replication instance* (or simply *replication instance*) is an ordered pair of study instances
- *s1* and *s2* refer to the first and second study of the pair; *sm* means the fixed-effect meta-analysis of the two studies
- $n1$, $d1_{pop}$, $d1_{sdz}$ are the sample size, population effect size, and Cohen’s $d$ for the first study
- $n2$, $d2_{pop}$, $d2_{sdz}$ are the sample size, population effect size, and Cohen’s $d$ for the second study
- $dm_{sdz}$ is Cohen’s $d$ from the meta-analysis
- *pairwise replication set* (or simply *replication set*) is the set of replication instances for a given $n1$, $n2$, $d1{pop}$, and $d2_{pop}$
- A replication instance or set is *exact* if the two studies are sampling the same population; for our simulated replications, this means $d1_{pop}=d2_{pop}$
- The terms *true* and *false* refer to the answers given by a correctness criterion when applied to a replication instance
    - *true instance* is a replication instance for which the criterion returns a true value
    - *false instance* is a replication instance for which the criterion returns a false value
- The terms *positive* and *negative* refer to the results of applying measures to replication instances
    - *positive instance* is a replication instance for which the measure reports a true value
    - *negative instance* is a replication instance for which the measure reports a false value
- The terms *true positive*, *false positive*, *true negative*, and *false negative* combine the notions of true vs. false and positive vs. negative instance. For a given correctness criterion and measure
    - *true positive instance* is a replication instance for which the correctness criterion and measure both return true values
    - *false positive instance* is a replication instance for which the correctness criterion is false but the measure returns true
    - *true negative instance* is a replication instance for which the correctness criterion and measure both return false values
    - *false negative instance* is a replication instance for which the correctness criterion is true but the measure returns false

For measures, the suffixes *1*, *2*, *m* mean respectively parameters or statistics from the first or second study of a replication instance or fixed-effect meta-analysis of the two studies.
- *sig* plus suffix means the p-value is significant for the indicated study or meta-analysis
- *d* plus suffix means $d_{sdz}$ for the indicated study or meta-analysis
- *c* plus suffix means the confidence interval for the indicated study or meta-analysis
- *p* plus suffix means the prediction interval for the indicated study or meta-analysis
- *scp* plus suffix refers to Uri's small telescope threshold
- *scpd* plus suffix is my extension to Uri's small telescope method that takes into account $d_{sdz}$ for the indicated study
- *sdir* means $d1_{sdz}$ and $d2_{sdz}$ are in the same direction, i.e., have the same sign
- *big1*, *big2* means $d1_{sdz} \ge d2_{sdz}$ or $d2_{sdz} \ge d1_{sdz}$, respectively
- *.* combines two statistics to yield a complete measure. For example, *d1.c2* means that $d_{sdz}$ from the first study is contained in the confidence interval from the second; *c1.c2* means the confidence intervals of the two studies overlap

For correctness criteria
- *non-zero* is *true* if $d1_{pop}=0$; with tolerance $\epsilon\ge0$, the result is *true* if $d1_{pop}\ge\epsilon$
- *same-effect* is *true* if $d1_{pop}=d2_{pop}$; with tolerance $\epsilon\ge0$, the result is *true* if $abs(d1_{pop}-d2_{pop})\ge\epsilon$

## Results

I make some assumptions to simplify the presentation.

- *s1* (the first study of the replication instance) represents the original study and *s2* (the second study) represents the replication
- the replication satisfies *sig1* and *sdir*, i.e., *s1* has a significant p-value and $d1_{sdz}$ and $d2_{sdz}$ are in same direction
- for small telescope measures, the replication also satisfies *sig2*, since the method as published requires this
- error rates are relative to *sig1*; e.g, an error rate of 5% means 5% of the replications satisfying *sig1* exhibit the error

### Exact Replications

A replication is *exact* if the two studies are sampling the same population, i.e., $d1_{pop}=d2_{pop}$. This is the obvious replication scenario. You have a study that you think may be a false positive; to check it out, you repeat the study, typically with a larger sample size, taking care to ensure that the replica closely matches the original.

For exact replications, *non-zero* is the only correctness criterion that matters; *same-effect* is true by construction.

A *false positive* occurs when *non-zero* is false (i.e., $d1_{pop}=0$), but the measure returns true. A *false negative* occurs when *non-zero* is true (i.e., $d1_{pop}\neq0$), but the measure returns false.

Figures 1-4 show false positive rates for a range of sample sizes.  The first two are line graphs displaying the measures that seem most important based on what I see in the literature; the latter two are heatmaps displaying all reasonable measures (everything except *sig1*, *sdir*, *big1*, *big2*). I sorted the rows of the heatmaps to get the best performing measures at the bottom; this produces nicer pictures, but causes the order of measures to be slightly different in the two figures.

Figures 1 and 3 are for $n1=20$ and $n2$ ranging from 50 to 500 in steps of 50; figures 2 and 4 are for $n1=40$ and $n2$ ranging from 100 to 1000 in steps of 100. Note that the smallest $n2$ in each case satisfies the recommendation in [Uri's post on the 90x75x50 heuristic](http://datacolada.org/54) that $n2=2.5 \times n1$ is big enough.

```{r out.width="50%",echo=FALSE,fig.show='asis'}
## TODO: change to correct file names!!
knitr::include_graphics('figure/plot_byprop.png');
knitr::include_graphics('figure/plot_byd.png');
knitr::include_graphics('figure/plot_vsprop.png');
knitr::include_graphics('figure/plot_vsd.png');
```

TODO: some words explaining the graph notation and what it all means.

Plotting false negatives is more tiresome, because we can also vary $d1_{pop}$ (and simultaneously $d2_{pop}$, of course). Figures 5-7 are heatmaps of false negative rates for the larger range of sample sizes above ($n1=40$ and $n2$ ranging from 100 to 1000), for $d1_{pop}=d2_{pop}$ varying over $\{0.2, 0.5, 0.8\}$. As one would expect, performance improves as the effect size grows. The heatmaps for the larger effect sizes show a lot of red, telling us that many measures have acceptable false negative rates.

For a measure to be usable, both error rates have to be acceptable.  Figures 8-10 look at false positive and false negative error rates side-by-side. The false positive heatmap is in the middle. The false negatives are for the two larger effect sizes: 0.5 and 0.8. Instead of sorting the measures to make pretty pictures, I plot them in the same order so you can compare them by eye. 

The only measure that is mostly red across all three figures is *sig2*. 

Figure XXX looks at the question more broadly using a concise rate-vs.-rate graph. This graph, inspired by *receiver operating characteristic (ROC)* curves, plots false negative vs. false positive rates for a large range of conditions. The data in this figure extends the cases plotted above: $n1$ varies over $\{20, 40,80, 160\}$, $n2$ ranges from $2.5 \times n1$ to $10\times$ in 10 steps, $d1_{pop}$ and $d2_{pop}$ range from 0 to 1 in steps of 0.1 with the constraint $d1_{pop}=d2_{pop}$. Each point shows the mean false negative vs. mean false positive rate for these conditions grouped by $n2$. The rationale for grouping by $n2$ is that operationally, when designing a replication, $n2$ is the only parameter you can control. The dashed lines demark the conventionally acceptable error rates: 0.05 for false positives and 0.2 for false negatives; the gray shading is the region where both error rates are acceptable. 

The graph is ugly but the message clear: *sig2* is the only measure with points falling in the acceptable range.

Boiling it down: *sig2* is the only game in town for exact replications. Its false positive rate is the significance level divided by 2; the factor of 2  comes from *sdir*, which knocks out half the false positives. The false negative rate is just $1-power$. In case there's any doubt about this last point, figure XXX plots *sig2*'s false negative rate and $1-power$ for a range of conditions.

TODO: compare scopes to *sig2*
But remember that the scopes **assume** *sig2*, so what we're really seeing is *sig2* in disguise. Running the scopes without *sig2* yields very high false positive rates (data not shown).

### Inexact replications

A replication is *inexact* if the two studies are sampling different populations, i.e., $d1_{pop} \ne d2_{pop}$. This scenario seems wierd to me. Since the populations are different, there's no reason to expect the studies to get similar answers. How then can a "replication failure" - however defined - invalidate the first study? What motivates this scenario, I think, is *generalizability* not validity. You have a study that demonstrates an interesting effect in a limited setting; you now want to know whether it generalizes to other settings.

If I were starting from first principles, I'd skip this case, because it has little to do with testing the validity of an initial study. However, since we've seen that none of the new replication-test methods work for exact replications, I decide to push on and see if they work here.

For inexact replications, both correctness criteria are relevant. There are more cases to consider than for exact replications since $d1_{pop}$ and $d2_{pop}$ can vary independently.

I'll look first at *non-zero*. Figures XXX-YYY plot false positives for a few conditions. *sig2* works fine when $d2_{pop}$ is small, but degrades as $d2_{pop}$ increases. The other measures are poor across the board. Figures XXX-YYY plot false negatives for similar conditions and a couple more.  Here performance is fine for some cases.  It doesn't matter, though, since the false positive rate is so bad.

I'm going to skip the heatmaps and go straight to a rate-vs.-rate graph.  The data is the same as I used for the *exact* case without the constraint that $d1_{pop}=d2_{pop}$. The message is the same as from the line graphs: several measures have fine false negative rates but the false positive rates are dreadful. Limiting ourselves to near-exact cases -- ones where the difference in effect size is $\le 0.1$ -- *sig2* performance is close to acceptable in a few cases. 

Moving on to *same-effect*. Figures XXX-YYY plot false positives for a few conditions. Again, performance is terrible across the board. Figures XXX-YYY plot false negatives for similar conditions. As for *non-zero*, false negative rate is fine for some cases, but it doesn't matter since the false negative rate is so bad. The rate-vs.-rate graphs in figures XXX-YYY tell the same story.  Adding some wiggle room by setting the tolerance greater than 0 helps a little but not enough.

## Dicussion

## Literature

The following papers and blog posts are the main ones that led me to the work here.

- The Science paper by the Open Science Collaboration "Estimating the reproducibility of psychological science", [accessible here](http://science.sciencemag.org/content/349/6251/aac4716) **if you have a Science account**, the [Comment by Gilbert et al](http://science.sciencemag.org/content/351/6277/1037.2.full), the [authors' response](http://science.sciencemag.org/content/351/6277/1037.3.full), [Uri Simonsohn's blog post](http://datacolada.org/47) commenting on the paper and response, and [Daniel Lakens's post on the response](http://daniellakens.blogspot.com/2016/03/the-statistical-conclusions-in-gilbert.html)
- The [Many Labs paper](https://econtent.hogrefe.com/doi/full/10.1027/1864-9335/a000178) by Klein and many others, the many [published comments](https://econtent.hogrefe.com/doi/pdf/10.1027/1864-9335/a000202), and [Uri's post](http://datacolada.org/63)
- [Uri's Small Telescopes paper](http://datacolada.org/wp-content/uploads/2015/05/Small-Telescopes-Published.pdf), and posts on [methods for evaluating replications](http://datacolada.org/47), [the 90x75x50 heuristic for setting replication sample sizes](http://datacolada.org/54), [accepting the NULL](http://datacolada.org/42), [the difficulty of accurately estimating effect size](http://datacolada.org/20), [the difficulty of evaluating replications](http://datacolada.org/7), and [significant results overestimate effect size](http://datacolada.org/4)
- Daniel Lakens's posts on [capture percentages](http://daniellakens.blogspot.com/2016/03/the-difference-between-confidence.html) and [cost/benefit analysis of replications](http://daniellakens.blogspot.com/2018/01/)
- [Lebel et al's A Guide to Evaluate Replications](https://osf.io/bku9j/)
- [Anderson and Maxwell's paper on replication goals](http://www.apa.org/pubs/journals/features/met-met0000051.pdf) and the [Replication Network blog post](https://replicationnetwork.com/2017/02/28/anderson-maxwell-theres-more-than-one-way-to-conduct-a-replication-study-six-in-fact/) commenting on the paper
- Papers on prediction intervals by [Patil, Peng, and Leek](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4968573/) and [Stanley and Spence](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0162874)
- [Sabeti's counterpoint in the Boston Globe](http://www.bostonglobe.com/ideas/2018/01/21/for-better-science-call-off-revolutionaries/8FFEmBAPCDW3IWYJwKF31L/story.html)
- Several articles from the Sackler Colloquium on Improving the Reproducibility of Scientific Research [published in PNAS](http://www.pnas.org/content/115/11), notably [Fanelli's opinion piece](http://www.pnas.org/content/115/11/2628) and [Shiffrin, Börner, and Stigler's big picture view](http://www.pnas.org/content/115/11/2632)


## SCRAPS

Replication Crisis is a great tag line.  It's a message that resonates with scientists and the general public alike. It implies that much of science is junk, a waste of taxpayer money, and for many voters, elitist noise that can be safely ignored when making decisions about, say, climate change, evolution, or anything else.

The Replication Crisis storyline goes something like this: (1) many published papers are wrong; (2) this is an outrage that the community must fix; and (3) systematic replication is an effective solution. (In recent months, I've seen an uptick in pre-registration as another solution. That's a topic for another day.)

Why are so many papers wrong? The story is that sample sizes are too small, and investigators resort to p-hacking or other statictical cheating to push the p-value across the goal line. Why is this an outrage? No one ever seems to give an explicit   answer, relying instead on the basic moral notion that lies are bad and truth is good. And how will replication fix the problem? Again no explicit answer.

The story never seems to go deeper. Why do scientists use such small samples? Are they stupid, incompetent, fraudsters, or too resource-limited to afford bigger studies? Nor does the story extend further in time. Is the long term plan to replicate all published work? That sounds pretty expensive: who'll pay for it? Or is the hope that once the community sees the problem they'll insist on better research practices? If so, what are those better practices and how do we know they'll fix the problem?


**BELOW HERE NOT PORTED**

Is science broken? A lot of people seem to think so, including some esteemed statisticians. One line of reasoning uses the concepts of false discovery rate and its complement, positive predictive value, to argue that most (or, at least, many) published scientific results must be wrong unless most hypotheses are *a priori* true. 

The *false discovery rate* (*FDR*) is the probability that a significant p-value indicates a false positive, or equivalently, the proportion of significant p-values that correspond to results without a real effect. The complement, *positive predictive value* ($PPV=1-FDR$) is the probability that a significant p-value indicates a true positive, or equivalently, the proportion of significant p-values that correspond to results with real effects.

I became interested in this topic after reading [Felix Schönbrodt's blog post](http://www.nicebread.de/whats-the-probability-that-a-significant-p-value-indicates-a-true-effect/), "What’s the probability that a significant p-value indicates a true effect?" and playing with his [ShinyApp](http://shinyapps.org/apps/PPV/). Schönbrodt's post led me to [David Colquhoun's paper](http://rsos.royalsocietypublishing.org/content/1/3/140216), "An investigation of the false discovery rate and the misinterpretation of p-values” and blog posts by [Daniel Lakens](http://daniellakens.blogspot.de/2015/09/how-can-p-005-lead-to-wrong-conclusions.html), “How can p = 0.05 lead to wrong conclusions 30% of the time with a 5% Type 1 error rate?” and [Will Gervais](http://willgervais.com/blog/2014/9/24/power-consequences), “Power Consequences”.

The term *science-wise false discovery rate* (repwr) is from [Leah Jager and Jeffrey Leek's paper](http://doi.org/10.1093/biostatistics/kxt007), "An estimate of the science-wise false discovery rate and application to the top medical literature". Though I didn’t realize it at the time, [John Ioannidis’s landmark paper](http://dx.plos.org/10.1371/journal.pmed.0020124), “Why most published research findings are false”, is the origin of it all.

## Scenario

Being a programmer and not a statistician, I decided to write some R code to explore this topic on simulated data. 

The program simulates a large number of problem instances representing published results, some of which are true and some false. The instances are very simple: I generate two groups of random numbers and use the t-test to assess the difference between their means. One group (the control group or simply *group0*) comes from a standard normal distribution with $mean=0$. The other group (the treatment group or simply *group1*) is a little more involved:

* for *true* instances, I take numbers from a standard normal distribution with mean *d* ($d>0$);
* for *false* instances, I use the same distribution as *group0*.

The parameter *d* is the effect size, aka *Cohen’s d*. 

I use the t-test to compare the means of the groups and produce a p-value assessing whether both groups come from the same distribution. 

The program does this thousands of times (drawing different random numbers each time, of course), collects the resulting p-values, and computes the FDR. The program repeats the procedure for a range of assumptions to determine the conditions under which most positive results are wrong.

For *true* instances, we expect the difference in means to be approximately *d* and for *false* ones to be approximately 0, but due to the vagaries of random sampling, this may not be so. If the actual difference in means is far from the expected value, the t-test may get it wrong, declaring a *false* instance to be positive and a *true* one to be negative. The goal is to see how often we get the wrong answer across a range of assumptions.

## Nomenclature

To reduce confusion, I will be obsessively consistent in my terminology.

- An *instance* is a single run of the simulation procedure.
- The terms *positive* and *negative* refer to the results of the t-test. A *positive instance* is one for which the t-test reports a significant p-value; a *negative instance* is the opposite. Obviously the distinction between positive and negative depends on the chosen significance level.
- *true* and *false* refer to the correct answers. A *true instance* is one where the treatment group (*group1*) is drawn from a distribution with $mean=d$ ($d>0$). A *false instance* is the opposite: an instance where *group1* is drawn from a distribution with $mean=0$.
- *empirical* refers to results calculated from the simulated data, as opposed to *theoretical* which means results calculated using standard formulas.

The simulation parameters are

| parameter |     meaning     | default |
| ------------- | -------------------------------------------- | ------------- |
| prop.true | fraction of cases where there is a real effect | `seq(.1,.9,by=.2)` |
| m | number of iterations | `1e4` |
| n | sample size | `16` |
| d | standardized effect size (aka *Cohen's d*) | `c(.25,.50,.75,1,2)` |
| pwr | power. if set, the program adjusts *d* to achieve power | `NA` |
| sig.level | significance level for power calculations when *pwr* is set | `0.05` |
| pval.plot | p-values for which we plot results | `c(.001,.01,.03,.05,.1)` |
 
## Results

The simulation procedure with default parameters produces four graphs similar to the ones below. 

```{r out.width="50%",echo=FALSE,fig.show='asis'}
knitr::include_graphics('figure/plot_byprop.png');
knitr::include_graphics('figure/plot_byd.png');
knitr::include_graphics('figure/plot_vsprop.png');
knitr::include_graphics('figure/plot_vsd.png');
```

In these graphs,

-  solid lines show theoretical results; dashed lines are empirical results from the simulation
-  *fdr*. false discovery rate
-  *pval*. p-value cutoff for significance
-  *prop.true*. proportion of true instances, i.e., ones that have a real effect
-  *d*. standardized effect size, aka *Cohen's d*

The first graph shows FDR vs. p-value across a range of *prop.true* values for a single effect size ($d=1$). Note the difference in *x* (p-value) and *y* (FDR) scales; the p-value scale is roughly an order of magnitude smaller than FDR. For this effect size, FDR behaves pretty well: for $prop.true=0.5$, FDR and p-value are pretty close; as *prop.true* gets smaller, FDR becomes larger than p-value; as *prop.true* gets larger, FDR shrinks below p-value. In other words, for this effect size, if most instances are true, p-values do a good job of separating the wheat from the chaff, but if most are false, p-values are less helpful. In the worse case plotted here, FDR is about 0.36 when $pval=0.05$.

The second graph shows FDR vs. p-value across a range of effect sizes for a single value of *prop.true* (0.5). Again note the difference in scales. Recall that FDR behaves pretty well for this value of *prop.true* when $d=1$. It’s still reasonable for $d=0.75$. But for smaller effect sizes, FDR again grows to be much larger than p-value. In the worse case plotted here, FDR is about 0.33 when $pval=0.05$.

We can also think of this in terms of power. As *d* gets smaller, so does power. The table below shows power for the default values of *d*. You’ll notice that power ranges from whopping good to anemic as we move from $d=2$ to $d=0.25$. For $d=0.75$, power is just over 50%; at this power, FDR is about .08 when $pval=0.05$. The table below shows FDR for all values of *d* under the conditions plotted here.

| *d*   | 0.25 | 0.50 | 0.75 | 1.00 | 2.00   |
| ---   | ---- | ---- | ---- | ---- | :----: |
| power | 0.10 | 0.28 | 0.54 | 0.78 | 0.9998 |
  
The third graph shows FDR vs. *prop.true* across a range of p-values for a single effect size ($d=1$). In this graph, the *x* and *y* scales are about the same. For this effect size, FDR behaves pretty well until *prop.true* gets below 0.3. The inflection point at 0.3 is an artifact of the simulation; adding a few more *prop.true* values between 0.1 and 0.3 smooths out the curve (data not shown).

The final graph shows FDR vs. *d* across a range of p-values for a single value of *prop.true* (0.5). As *d* drops below 1, FDR grows rapidly as we’ve seen before. Reducing the p-value helps, as you would expect. But even with p-value=.001, FDR grows rapidly for $d<0.5$, reaching about 0.2 for $d=0.25$. This is because power is abysmal (.004) at this point causing us to miss most true instances. This illustrates the tradeoff between false positives and false negatives as we reduce the p-value: smaller p-values give fewer false positives but also fewer true positives.

Returning to the second graph above (FDR vs. p-value for a range of effect sizes and $prop.true=0.5$), we see that for small values of *d* and *pval*, the empirical results are noisy and don't match the theoretical results very well. This is because there aren’t enough positives in this region. Increasing the number of simulations to $10^6$ fixes the problem as shown in the graph below.

```{r out.width="50%",echo=FALSE,fig.show='asis'}
knitr::include_graphics('figure/plot_byd.m=1e6.png');
```

The relationship between FDR and p-value is complicated. If *prop.true* is 50% or better and *d* is 1 or more, p-values do a good job at discriminating true from false instances. Under less optimistic conditions, p-values are not so good. Under the most pessimistic conditions here, FDR is about 1/3. Reducing the significance level improves FDR but at the cost of missing more true instances.

Let’s look at extreme cases of *prop.true* (0.25, 0.75) and power (0.2, 0.8) for *pval=.05*.  The table below shows theoretical FDR for these cases.

```{r eval=FALSE,include=FALSE}
source("script/repwr.R");
init(prop.true=c(.25,.75),pwr=c(.2,.8),pval.interp=.05)
dosim();
dointerp();
interp$pwr=with(interp,power.t.test(n=n,delta=d)$power); 
table=unstack(interp,fdr.theo~prop.true);
table=table[order(table[,1]),order(table[1,])];
rownames(table)=paste(sep=' ',c('high','low'),'prop.true');
colnames(table)=paste(sep=' ',c('high','low'),'power');
round(table,2)
```

|                | high power | low power |
| -------------- | :--------: | :-------: |
| **high prop.true** |     0.02   |  0.16     |
| **low prop.true**  |     0.08   |  0.43     |

The best case is great (FDR=0.02), the worst case is horrible (0.43), and the in-between cases range from 0.08 to 0.16. The take-home is that if most hypotheses are wrong, you have to do good, well-powered studies to find the few correct results, but if most hypotheses are correct, you may be able to get by with sloppy science.

## Discussion

I started with the question, “Is science broken?” and segued to the more specific question of “Are most (or, at least many) published results wrong?”  I then reported the claim that "Yes, most (or many) results must be wrong unless most hypotheses are *a priori* true", because the science-wise false discovery rate (repwr) makes it so. Do the results here support the claim?

It depends on *prop.true*, so we’d better be clear about what it represents.

* [David Colquhoun's paper](http://rsos.royalsocietypublishing.org/content/1/3/140216) seems to suggest that it refers to early stage experiments. At one point the paper says, “[I]magine that we are testing a lot of candidate drugs, one at a time. It is sadly likely that many of them would not work, so let us imagine that 10% of them work and the rest are inactive." In the on-line post-publication discussion, Dr. Colquhoun is even more explicit: “To postulate a prevalence greater than 0.5 is tantamount to saying that you were confident that your hypothesis was right before you did the experiment.”

* [Felix Schönbrodt's blog post](http://www.nicebread.de/whats-the-probability-that-a-significant-p-value-indicates-a-true-effect/) has a similar statement: “It’s certainly not near 100% – in this case only trivial and obvious research questions would be investigated, which is obviously not the case.”

I think this interpretation is dead-wrong. Hypotheses exist at many stages of research from vague ideas flitting through students' heads to precise claims in published papers. Since we’re reasoning about the validity of **published** results, *prop.true* (and other parameters like *d*) must refer to hypotheses late in the research process, ones that are far enough along to be considered for publication. Presumably, the research process weeds out many incorrect hypotheses; ones that make it this far are more likely to be true. I don’t know what the right number is, but it must be higher than the estimates proposed by Colquhoun and Schönbrodt.

What happens to the incorrect hypotheses that make it to the near-publication stage? I see three possibilities:

1. By (bad) luck, the study yielded a significant p-value, and the happy but hapless investigators proceed to publication.
2. The lab chief thinks the negative finding is correct and publishes the negative result or abandons the work. Sadly, this happens rarely, as we know too well.
3. The lab chief is unconvinced and sends the student back to the lab for more experiments or to the computer for more analyses. This is p-hacking and will likely repeat until the student gets a positive result or the professor gives up.

Ignoring the rare case #2, all false hypotheses that make it this far will eventually yield positive results and be published. This makes the work we’ve done simulating FDR totally irrelevant. The FDR we get will be close to whatever value we assume for the proportion of false hypotheses. In other words, $FDR \approx 1-prop.true$. Rather obvious, I think, and completely pointless.

I’ve seen plenty of bad science up close and personal and am thoroughly convinced that many published results in my field are rubbish. But I don’t buy the arguments based on repwr. The problem is p-hacking, both experimental and computational.

It's really just another consequence of "Publish or Perish".  Those who can, publish good science; those who can't, p-hack.  No amount of statistical cleverness can change this basic dynamic. If you replace the much-maligned p-value by some other statistic *s*, p-hackers will become s-hackers, and the overall quality of science will remain unchanged. 

Good science drives the field forward; bad science is ephemeral. I know it's aggravating to see so much dreck get published, but it's even more aggravating to see good statisticians and data scientists agonizing over the ordure and spending so much effort trying to root out bad science. We will do more good by helping good scientists do good science than by trying to slow down the bad ones. Quoting the sage Oprah Winfrey ([from BrainyQuote](https://www.brainyquote.com/quotes/quotes/o/oprahwinfr163087.html)), "Be thankful for what you have; you'll end up having more. If you concentrate on what you don't have, you will never, ever have enough." 
